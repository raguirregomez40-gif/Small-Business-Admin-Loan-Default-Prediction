import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# loading the data
df_1990s = pd.read_csv("foia-7afy1991-fy1999-asof-221231.csv", encoding='latin1', low_memory = False)
df_2000s = pd.read_csv("2000-2009.csv", encoding='latin1', low_memory = False)
df_2010s = pd.read_csv("2010-2019.csv", encoding='latin1', low_memory = False)

# Joining the 2000s and 2010s data sets
df = pd.concat([df_2000s, df_2010s], ignore_index=True)

# Check that its loaded
df.head()

# Checking the data's types and null counts
df_1990s.info()

# shape of main dataframe right after creation
print(f"The set has {df.shape[0]} lines")

# break suggestion for jupyter

print("This is the first set- 1990s")
print(df_1990s.shape)

print("Gross Approval Max for the 1990s")
GA90 = df_1990s["GrossApproval"].max()
print(GA90)

print("Min Gross Approval Max for the 1990s")
MINGA90 = df_1990s["GrossApproval"].min()
print(MINGA90)

print("Guaranteed Approval by the SBA for the 1990s")
SBAGA90 = df_1990s["SBAGuaranteedApproval"].max()
print(SBAGA90)

print("Min Guaranteed Approval by the SBA for the 1990s")
Min90 = df_1990s["SBAGuaranteedApproval"].min()
print(Min90)

CO1990 = df_1990s['LoanStatus'].value_counts(normalize=False)["CHGOFF"]

PIF1990 = df_1990s['LoanStatus'].value_counts(normalize=False)["PIF"]

ratio1990s = CO1990/PIF1990
print("Ratio of Charged Off to Paid in Full for the 1990s:", ratio1990s)
print(f"There are {CO1990} chargeoff loans to {PIF1990} paid in full loans")

# Check if previous commands changed anything on the main data frame
#print(f"Main data frame shape revision: {df.shape}")

# break suggestion 

print("This is the second set- 2000s")
print(df_2000s.shape)

print("Gross Approval Max for the 2000s")
GA00 = df_2000s["GrossApproval"].max()
print(GA00)

print("Min Gross Approval Max for the 2000s")
MINGA00 = df_2000s["GrossApproval"].min()
print(MINGA00)


print("Guaranteed Approval by the SBA for the 2000s")
SBAGA00 = df_2000s["SBAGuaranteedApproval"].max()
print(SBAGA00)

print("Min Guaranteed Approval by the SBA for the 2000s")
Min00 = df_2000s["SBAGuaranteedApproval"].min()
print(Min00)

CO2000 = df_2000s['LoanStatus'].value_counts(normalize=False)["CHGOFF"]

PIF2000 = df_2000s['LoanStatus'].value_counts(normalize=False)["PIF"]

ratio2000s = CO2000/PIF2000
print("Ratio of Charged Off to Paid in Full for the 1990s:", ratio2000s)
print(f"There are {CO2000} chargeoff loans to {PIF2000} paid in full loans")

print("This is the third set- 2010s")
print(df_2010s.shape)

print("Gross Approval Max for 2010s")
GA10 = df_2010s["GrossApproval"].max()
print(GA10)

print("Min Gross Approval Max for 2010s")
MINGA10 = df_2010s["GrossApproval"].min()
print(MINGA10)

print("Guaranteed Approval by the SBA for 2010s")
SBAGA10 = df_2010s["SBAGuaranteedApproval"].max()
print(SBAGA10)

print("Min Guaranteed approval by the SBA for 2010s" )
Min10 = df_2010s["SBAGuaranteedApproval"].min()
print(Min10)

CO2010 = df_2010s['LoanStatus'].value_counts(normalize=False)["CHGOFF"]

PIF2010 = df_2010s['LoanStatus'].value_counts(normalize=False)["PIF"]

ratio2010s = CO2010/PIF2010
print("Ratio of Charged Off to Paid in Full for the 1990s:", ratio2010s)

print(f"There are {CO2010} chargeoff loans to {PIF2010} paid in full loans")

# Check if previous commands changed anything on the main data frame
#print(f"Main data frame shape revision: {df.shape}")

# loop for checking every data frames missing values

for i, dft in enumerate([df_1990s,df_2000s,df_2010s],start = 1):
    print(f"\n Missing values in dataset {i}")
    print(dft.isnull().sum())
    
    
    print(f"Total rows with missing data: {dft.isnull().any(axis=1).sum()}")

    print(f"\nDuplicates: {dft.duplicated().sum()}")

# check how many values before we do anything to data set 
print (f" Making sure the previous code didn't change the main dataframe {df.shape}")


# Duplicates should be 112 289 1149 if data loaded right 


# DataFrame Info
print("Main Data Frame Info")
df.info()

# Checking/ revising the dataset
print("Data Frame Description")
print(df.describe(include='all'))

## dealing with duplicates, figure out how many there are in total in the main data set we will use 

# Checking the ratio of defaults to paid off before deleting duplicates.

# Number of default loans
COdf = df['LoanStatus'].value_counts(normalize=False)["CHGOFF"]

# Number of paid off loans in the main dataframe 
PIFdf = df['LoanStatus'].value_counts(normalize=False)["PIF"]

# getting ratio and displaying values
ratiodf = COdf/PIFdf

print("Ratio of Charged Off to Paid in Full for the df before duplicate deletion:", ratiodf)

print(f"There are {COdf} chargeoff loans to {PIFdf} paid in full loans before duplicate deletion")

#duplicate deletion
A = df.shape[0]

# number of rows before deletion
print(f"Rows before duplicate deletion: {A}")

df = df.drop_duplicates()

#number of rows after deletion
B = df.shape[0]
print(f"Rows after duplicates deletion {B}")

print(f"Their difference is {A-B} Rows deleted")

# checking ratio after deletion

COdf = df['LoanStatus'].value_counts(normalize=False)["CHGOFF"]

PIFdf = df['LoanStatus'].value_counts(normalize=False)["PIF"]

ratiodf = COdf/PIFdf
print("Ratio of Charged Off to Paid in Full for the df after duplicate deletion:", ratiodf)

print(f"There are {COdf} chargeoff loans to {PIFdf} paid in full loans after duplicate deletion")



# sanity check
print(df.shape)

print("Data Frame's null values")
df.isnull().sum()

# Deleting the columns we won't use 

# Creating the list of columns we will delete

useless_cols=['AsOfDate','Program','BankNCUANumber','FranchiseCode',
             'FranchiseName', 'BusinessAge', 'InitialInterestRate', 
              'ChargeOffDate','PaidInFullDate','BorrName','BorrStreet',
              'BorrZip','BankStreet','BankZip','ApprovalDate','DeliveryMethod',
                'GrossChargeOffAmount']

# Actually dropping the columns
df.drop(columns=[col for col in useless_cols if col in df.columns], inplace=True) #I added this so we can keep rerunning this cell without errors

df = df.copy() # i Added this as a checkpoint 

# Changing the missing values to ones we can use 
df['NaicsDescription']   = df['NaicsDescription'].fillna('Unknown')
df['BankFDICNumber']     = df['BankFDICNumber'].fillna(0)
df['FirstDisbursementDate'] = df['FirstDisbursementDate'].fillna('Unknown')
df['BusinessType']       = df['BusinessType'].fillna(df['BusinessType'].mode()[0])

# sanity check
print(" Full Data frame size")
print(df.shape)
print(df.info())

# See what categories are present in the loan status column
print(df["LoanStatus"].unique())
#removed the loans that were either, canceled, exempt or Commit, they dont fit our criteria leaving us iwth 947,339 lines of code
df = df[df["LoanStatus"].isin(["PIF", "CHGOFF"])]
#checks how many lines are left
print(df.shape)
#checks that the non-null values stays at the same amount as the number of lines
df.info()

status_map = {'PIF': 1, 'CHGOFF': 0}
df['LoanStatus'] = df['LoanStatus'].map(status_map)

# check to make sure the non- null values are the same and dont get messed up into all null 
df.info()

#checking how many have null values 
df.isnull().sum()

## add a check to see how many lines before the na drop

# Getting values of default vs paid off for the clean data set 
COdf = df['LoanStatus'].value_counts(normalize=False)[0]

PIFdf = df['LoanStatus'].value_counts(normalize=False)[1]

# Ratio the counts to see the ratio of the full data frame then print 
ratiodf = COdf/PIFdf
print("Ratio of Charged Off to Paid in Full for the df before NaN deletion:", ratiodf)

print(f"There are {COdf} chargeoff loans to {PIFdf} paid in full loans before NaN deletion")

#Checking the rows before we do the null deletion
germ = df.shape[0]
print(f"Rows before NaN deletion: {germ}")

#Dropping the null values 
df = df.dropna()

# Checking the amount of lines we have after null deletion 
worm = df.shape[0]
print(f"Rows after NaN deletion {worm}")

# Difference in instances before and after null deletion
print(f"Their difference is {germ-worm} Rows deleted")

# Can do sanity check here 
#df.info()

# Ratio of paid off to defaults after null deletion
COdf = df['LoanStatus'].value_counts(normalize=False)[0]

PIFdf = df['LoanStatus'].value_counts(normalize=False)[1]

# Ratio creation and display
ratiodf = COdf/PIFdf
print("Ratio of Charged Off to Paid in Full for the df after NaN deletion:", ratiodf)

print(f"There are {COdf} chargeoff loans to {PIFdf} paid in full loans after NaN deletion")

# Simple check
df.info()

# Making sure we have no null values left 
df.isnull().sum()

# Describe the cleaner data set 
df.describe()

print(" Data frame shape: ", df.shape)


# Max Gross Approval and SBA Guranteed approval for the main cleaner data set 
GAdf = df["GrossApproval"].max()

print(GAdf)

SBAGAdf = df["SBAGuaranteedApproval"].max()

print(SBAGAdf)

top_n = 5  # how many most frequent categories we want to show

for col in df.columns:
    print(f"\nColumn: {col}")
    if df[col].dtype == 'object' or df[col].dtype.name == 'category':
        print(df[col].value_counts().head(top_n)) # shows n top categories with the most values
        print(df[col].nunique()) # gives us number of unique values 
        print(df[col].unique()) # Shows what the values in the column look like 
    else:
        print("Numeric column â€” skipped") # skip the numeric, there can be infinite categories 


matrix = np.array([[GA90,GA00,GA10],
                   [MINGA90,MINGA00,MINGA10],
                   [SBAGA90,SBAGA00,SBAGA10],
                   [Min90,Min00,Min10]])

kidf = pd.DataFrame(matrix, 
                   index = ["Gross Approval Max","Min Gross Approval","Max SBA Guaranteed Approval","Min SBA Guaranteed Approval"],
                   columns = ['1990s','2000s','2010s'])

print(kidf)

kidf_long = kidf.reset_index().melt(
    id_vars='index', 
    var_name='Period', 
    value_name='Value'
)
print(kidf_long)

kidf_long.rename(columns={'index': 'Variable'}, inplace=True)

print(kidf_long)

sns.barplot(x='Variable', y='Value', hue='Period', data=kidf_long)

plt.title("Gross and SBA Guaranteed Approvals by Time Period")
plt.xlabel("Variable")
plt.ylabel("Value")
plt.xticks(rotation=20)
plt.legend(title="Period")
plt.show()

# Histograms for useful numeric variables
histcols =  ['GrossApproval', 'SBAGuaranteedApproval', 'ApprovalFiscalYear', 'TermInMonths', 'LoanStatus', 'RevolverStatus', 'JobsSupported']
plt.figure(figsize=(14, 10))
for i, col in enumerate(histcols, 1):
    plt.subplot(3, 3, i)
    sns.histplot(df[col], bins=30, kde=True, color='skyblue')
    plt.title(col)
    plt.xlabel('')
    plt.ylabel('Count')

plt.tight_layout()
plt.ticklabel_format(style='plain', axis='both')
plt.show()


# Heatmap Correlation Matrix
numeric_df = df.select_dtypes(include='number')
numeric_df = numeric_df.drop(columns=['BankFDICNumber', 'ApprovalFiscalYear', 'NaicsCode', 'CongressionalDistrict'])
fig, ax1 = plt.subplots(figsize=(11, 11))
sns.heatmap(numeric_df.corr(), annot=True, ax=ax1)


#Univrate graphs 
numeric_cols = ['GrossApproval', 'SBAGuaranteedApproval', 'TermInMonths', 'JobsSupported']

# Histograms for numeric variables
plt.figure(figsize=(14, 10))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(3, 3, i)
    sns.histplot(df[col], bins=40, kde=True, color='steelblue')
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
plt.tight_layout()
plt.savefig("univariate_distributions.png", dpi=300, bbox_inches='tight')
plt.show()


import catboost
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

X = df.drop(columns = ["LoanStatus"])
y = df["LoanStatus"]

cat_features = X.select_dtypes( include = ["object", "category"]).columns.tolist()

text_features = ['NaicsDescription'] if 'NaicsDescription' in cat_features else []

print(cat_features)
print(text_features)

cat_features = [c for c in cat_features if c not in text_features]

print(cat_features)

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .20 , random_state = 8, stratify = y)

model = CatBoostClassifier(cat_features = cat_features,
                           text_features = text_features,
                           iterations = 20,
                           learning_rate = .05,
                           depth = 8,
                           loss_function = "Logloss",
                           eval_metric = 'Accuracy',
                           verbose = 100)
model.fit(X_train, y_train,
          eval_set = (X_test, y_test),
          use_best_model = True)

y_pred = model.predict(X_test)

print("Accuracy: ", accuracy_score(y_test,y_pred))

print(classification_report(y_test,y_pred))

feat_imp = pd.DataFrame(model.get_feature_importance(prettified = True),columns = ['Feature Id','Importances'])

top10 = feat_imp.sort_values(by = 'Importances',ascending = False).head(10)

plt.figure(figsize = (8,5))
plt.barh(top10['Feature Id'],top10['Importances'],color = 'steelblue')

plt.gca().invert_yaxis()

plt.title('Top 10 most important features')
plt.ylabel("Importance Score")
plt.xlabel("Importance Feature")
plt.tight_layout()
plt.show()

print(feat_imp.columns)

model.get_feature_importance(prettified = True).head(10)




# Random Forest


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns


X = df[['GrossApproval', 'SBAGuaranteedApproval', 'TermInMonths','JobsSupported']]
y = df['LoanStatus']  


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


rf_model = RandomForestClassifier(
    n_estimators=20,
    random_state=42,
    class_weight='balanced'
)
rf_model.fit(X_train_scaled, y_train)


y_pred_rf = rf_model.predict(X_test_scaled)
y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]


print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("ROC AUC:", roc_auc_score(y_test, y_pred_proba_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

importances = rf_model.feature_importances_
features = X.columns

feat_importances = pd.DataFrame({'Feature': features, 'Importance': importances})
feat_importances = feat_importances.sort_values(by='Importance', ascending=False).head(10)

plt.figure(figsize=(8,5))
sns.barplot(x='Importance', y='Feature', hue='Feature', data=feat_importances, palette = 'mako')
plt.title('Feature Importances for Random Forest Model')
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.savefig('rf_feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()


from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    'n_estimators': [100, 200, 300],      
    'max_depth': [8, 12, 16, None],      
    'max_features': ['sqrt', 0.4, 0.6],   
    'min_samples_split': [2, 5, 10],     
    'min_samples_leaf': [1, 2, 4],       
    'bootstrap': [True],               
    'criterion': ['gini', 'entropy'] 
}

random_search = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_dist,
    n_iter=10,         
    scoring='f1',      
    cv=2,               
    verbose=2,
    random_state=42,
    n_jobs=-1
)

random_search.fit(X_train, y_train)

best_rf = random_search.best_estimator_
print("Best params:", random_search.best_params_)




# ==== Logistic Regression (no GrossChargeOffAmount) ====
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report

import matplotlib.pyplot as plt
import seaborn as sns


FEATURES = ['GrossApproval', 'SBAGuaranteedApproval', 'TermInMonths', 'JobsSupported']
TARGET   = 'LoanStatus'

dfm = df[FEATURES + [TARGET]].copy()


t = dfm[TARGET]
if pd.api.types.is_numeric_dtype(t):
    dfm[TARGET] = pd.to_numeric(t, errors='coerce').round().astype('Int64')
else:
    norm = (t.astype(str).str.strip().str.upper()
              .replace({'PAID IN FULL':'PIF','PAID-IN-FULL':'PIF',
                        'CHARGE OFF':'CHGOFF','CHARGEOFF':'CHGOFF'}))
    dfm[TARGET] = norm.map({'PIF':1, 'CHGOFF':0}).astype('Int64')


dfm = dfm[dfm[TARGET].isin([0,1])]


for c in FEATURES:
    dfm[c] = pd.to_numeric(dfm[c], errors='coerce')


X_raw = dfm[FEATURES]
y     = dfm[TARGET].astype(int)


imp = SimpleImputer(strategy='median')
X_imp = pd.DataFrame(imp.fit_transform(X_raw), columns=FEATURES, index=X_raw.index)


scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_imp), columns=FEATURES, index=X_imp.index)


use_strat = y.value_counts().min() >= 2
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=20, stratify=y if use_strat else None
)


log_reg = LogisticRegression(
    solver='liblinear',        
    class_weight='balanced',   
    max_iter=1000,
    random_state=20
)
log_reg.fit(X_train, y_train)
y_pred  = log_reg.predict(X_test)
y_prob  = log_reg.predict_proba(X_test)[:, 1]

print("Accuracy :", round(accuracy_score(y_test, y_pred), 4))
print("F1-score :", round(f1_score(y_test, y_pred), 4))
print("ROC AUC  :", round(roc_auc_score(y_test, y_prob), 4))
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))


coef = pd.Series(log_reg.coef_[0], index=FEATURES)
coef_df = (coef.rename('Coefficient')
               .reindex(coef.abs().sort_values(ascending=False).index) 
               .reset_index().rename(columns={'index':'Feature'}))

plt.figure(figsize=(9,5))
sns.barplot(x='Coefficient', y='Feature', data=coef_df, palette='viridis')
plt.title('Logistic Regression Coefficients (standardized inputs)')
plt.xlabel('Coefficient (log-odds per +1 SD)')
plt.ylabel('Feature')
plt.axvline(0, color='k', lw=1)
plt.tight_layout()
plt.savefig('logreg_coeffs.png', dpi=300, bbox_inches='tight')
plt.show()


